---
title: "BSMM-lab-2"
subtitle: "BSMM 8740 Fall 2023"
author: "Archana Vijayakumar Sreekala"
date: "September, 27 2023"
format: html
editor: visual
self-contained: true
---

## Setup

Load packages and data:

```{r load-pkg-data}
#| message: false

#install.packages("tidyverse")
#install.packages("tidymodels")
#install.packages("gt")
#install.packages("gtExtras")
#install.packages("DataExplorer")

library(magrittr)     # the pipe
library(tidyverse)    # for data wrangling + visualization
library(tidymodels)   # for modeling
library(gt)           # for making display tables
library(gtExtras)     # helper functions for beautiful tables
library(DataExplorer) #
library(tidyr)
library(dplyr)
library(readr)

the_tate <- readr::read_delim("data/the-tate-collection.csv", ";", escape_double = FALSE, trim_ws = TRUE)
the_tate_artists <- readr::read_csv("data/the-tate-artists.csv")
```

## Exercises

### Exercise 1

The `the_tate` dataset has 3336 unique artists who worked from 1545 to 2012. The works were acquired between the years 1823 and 2013.

```{r Data Analysis}

the_tate
the_tate_artists

dim(the_tate)
dim(the_tate_artists)

the_tate |> dplyr::slice_head(n=2) |> dplyr::glimpse()
the_tate_artists |> dplyr::slice_head(n=2) |> dplyr::glimpse() 

view(the_tate)
view(the_tate_artists)

summary(the_tate)

# Summarise function to group by each artist and find the summarised information 
the_tate |> dplyr::group_by(artist) |>
  dplyr::summarise(N=n(), 
         min_year = min(year, na.rm = TRUE),          
         max_year = max(year, na.rm = TRUE), 
         min_acquisition_year = min(acquisitionYear, na.rm = TRUE),
         max_acquisition_year = max(acquisitionYear, na.rm = TRUE)
  )

# To find the number of unique artists

unique_artist_count <- 
  the_tate |> select(artist) |>
         distinct() |>
         nrow() 
# To find the range of years that the artists worked
min_year <- min(the_tate$year, na.rm = TRUE)         
max_year <- max(the_tate$year, na.rm = TRUE)  
# To find the range of acquisition years
min_acquisition_year <- min(the_tate$acquisitionYear, na.rm = TRUE)
max_acquisition_year <- max(the_tate$acquisitionYear, na.rm = TRUE)

cat("The the_tate dataset has", unique_artist_count, "unique artists who worked from",
    min_year, "to", max_year, ".\n The works were acquired between the years", min_acquisition_year, "and", max_acquisition_year, ".\n")


DataExplorer::introduce(the_tate)
DataExplorer::introduce(the_tate_artists)

DataExplorer::plot_missing(the_tate) 
DataExplorer::plot_missing(the_tate_artists)
```

### Exercise 2

```{r number of works with missing dates}


Total_missing_date <- sum(is.na(the_tate$year))
cat("How many number of works with missing dates is", Total_missing_date,".\n")
```

The number of artists whose works have missing dates is \_\_.

```{r number of artists whose works have missing dates}

missing_dates_artists <- the_tate |> dplyr::filter(is.na(year)) |>
                         dplyr::distinct(artist) |>
                         nrow()
 
cat("The number of artists whose works have missing dates is",missing_dates_artists, ".\n")        
```

```{r count of works missing data against each artist arranged in desc order}


Artists_missing_dates <- the_tate |> dplyr::filter(is.na(year)) |>
                         group_by(artist) |>
                         summarise( Missing_dates = n())|>
                         arrange(desc(Missing_dates)) |>
                         as_tibble()
Artists_missing_dates
```

```{r Include percentage of missing dates per artists}

Artists_missing_dates <- Artists_missing_dates |> 
                         mutate(percentage_missing_dates = (Missing_dates/Total_missing_date)*100)

Artists_missing_dates
```

```{r To find the cumulative percentage against each artists}

Artists_missing_dates <- Artists_missing_dates |> 
                         mutate(cumulative_percentage = cumsum(percentage_missing_dates))

Artists_missing_dates
```

```{r smallest number of arists needed to resolve at least 50% of the missing year data}

artist_to_resolve_50_percent <- min(which(Artists_missing_dates$cumulative_percentage >= 50))

cat("It would require resolving missing year data for only", artist_to_resolve_50_percent, "artists to resolve at least 50% of the missing data.\n")

cat("\nThe missing year data likely to be classified as MAR")

```

### Exercise 3

```{r artist with most works in the tate collection}

artist_max_work <- the_tate |> group_by(artist) |>
                   summarise(no_of_works = n()) |>
                   arrange(desc(no_of_works))
artist_max_work

#artist_max_work |> select (artist) |> slice_head(n=1) 
```

```{r top 10 artists with most of the works}

top_ten_artists <- artist_max_work |> slice_head(n=10) 
top_ten_artists
```

The artist with the most works in the Tate collection is **Turner, Joseph Mallord William**.

The artist with the tenth-most works in the Tate collection is Warhol, Andy.

### Exercise 4

```{r percentage of the total collection that each artist represents}

# Total number of works
Total_no_of_works <- nrow(the_tate)
Total_no_of_works

# Percentage of total collection per artist
artist_max_work %<>%mutate(percentage=(no_of_works/Total_no_of_works)*100)
artist_max_work


# using gt
table_formatted <- artist_max_work %>%
  gt() %>%
  fmt_number(
    columns = c(no_of_works, percentage), # Format both no_of_works and percentage columns
    decimals = 2 # No decimal places for no_of_works, and decimals for percentage
  ) %>%
  tab_header(title = "Top Artists by Number of Works and Percentage of Collection")

# Print the formatted table
print(table_formatted)

```

The artist with the greatest number of works in the Tate collection represent **56.92%** of the total number of works

### Exercise 5

```{r to find duplicate records}

# select the columns for artist and title and count the number of rows

total_rows <- the_tate |> select(artist,title) |> nrow()

# Count the distinct artist-title pairs

 distinct_artist_title <- the_tate |> select(artist,title) |> distinct()
 distinct_rows <- nrow(distinct_artist_title)

# count the number of duplicated artist-title pair
 duplicate_count <- total_rows - distinct_rows
 cat ("There are", duplicate_count, "duplicate artist-title pairs .\n") 
```

### Exercise 6

```{r }

# add a column with area in cm2
table_area <- the_tate |> mutate(area_cm2 = (width * height)*0.01)
  
# select required fields and drop NA values
selected_records <- table_area |> select (artist, title, area_cm2) |>
                   drop_na()


# Order the work by area
ordered_records <- selected_records |> arrange(area_cm2)
ordered_records

# To find the largest artwork in the collection
largest_artwork <- ordered_records |> slice_tail(n=1)
largest_artwork

largest_work_artist <- largest_artwork |> select(artist)

# To find the smallest artwork in the collection
smallest_artwork <- ordered_records |> slice_head(n=1)
smallest_artwork
```

The artist with the largest work in the tate collection is Therrien, Robert.

The artist with the smallest work in the collection is Mesens, E.L.T. The smallest work has area 2.37 $\text{cm}^2$

### Exercise 7

```{r left join}

view(the_tate_artists)

# left join the tables, filter gender 'NA' and group by gender
leftjoin_op <- the_tate |> left_join(the_tate_artists,by = c("artistId" = "id")) |>
                           filter(!is.na(gender)) |> group_by(gender)

leftjoin_op
```

### Exercise 8

```{r}

SPX_History <- readr::read_csv("data/SPX_HistoricalData_1692322132002.csv")

# To add Year to the existing dataset
SPX_History <- SPX_History |> mutate(Year = lubridate::year(as.Date(Date,format = "%m/%d/%Y")))
 
 # To add the column for daily return, ri
 SPX_History <- SPX_History |> mutate(daily_return = log(lead(`Close/Last`)/`Close/Last`))
 
 #To add variance column
 SPX_History <- SPX_History |> mutate(variance = (daily_return)^2)
 
 view(SPX_History)     
 
 # To summarise the returns and variance annually
 summary_data <- SPX_History |> group_by(Year) |>
                summarise(annual_return= (exp(sum(daily_return, na.rm = TRUE)) -1) *100,
                          annual_stddev = sqrt(sum(variance, na.rm = TRUE)) * 100,
                          .groups = "drop" # Drop grouping after summary
                          )
                
summary_data 
```

The annual return in the SPX price in 2020 was -13.98%.

The corresponding price volatility was 34.70%.

### Exercise 9

```{r calculate period volatility based on sd fucntion}

volatility <- sd(summary_data$annual_return)
print(volatility)
```

The period volatility was 19.51%

```{r To find period volatility based on summary of annual stdcdev}

# Install packages for reading the csv file

# Load required libraries
library(dplyr)
library(readr)

# Calculate period return and period volatility
period_return <- prod(1 + summary_data$annual_return) - 1
period_volatility <- sqrt(sum(summary_data$annual_stddev^2))

# Create summary rows for period return and period volatility
summary_rows <- tibble::tibble(
  Year = as.character("Period"),  # Ensure "Year" is character type
  annual_return = period_return,
  annual_stddev = period_volatility
)
summary_rows

# Convert the "Year" column in summary_data to character
summary_data <- summary_data |>
  mutate(Year = as.character(Year))

# Combine the summary rows with the summary_data
summary_data <- bind_rows(summary_data, summary_rows)

# Print the summary data
print(summary_data) 
```

The period volatility was 48.8%
